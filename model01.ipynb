{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQMmL2AoTysy"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "np.random.seed(123)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import *\n",
        "import itertools\n",
        "\n",
        "import keras\n",
        "from keras.utils.np_utils import to_categorical # used for converting labels to one-hot-encoding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras import backend as K\n",
        "import itertools\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
        "from keras.applications import VGG19, ResNet50\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sigwaxKAT5WV"
      },
      "source": [
        "#1. Function to plot model's validation loss and validation accuracy\n",
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN5sDNnWT9Ik"
      },
      "source": [
        "Making Dictionary of images and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOIcDvWST-gV"
      },
      "source": [
        "base_skin_dir = os.path.join('..', 'input','skin-cancer-mnist-ham10000')\n",
        "\n",
        "# Merging images from both folders HAM10000_images_part1.zip and HAM10000_images_part2.zip into one dictionary\n",
        "\n",
        "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
        "                     for x in glob(os.path.join(base_skin_dir, '*', '*.jpg'))}\n",
        "\n",
        "# This dictionary is useful for displaying more human-friendly labels later on\n",
        "\n",
        "lesion_type_dict = {\n",
        "    'nv': 'Melanocytic nevi',\n",
        "    'mel': 'Melanoma',\n",
        "    'bkl': 'Benign keratosis-like lesions ',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratoses',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jieJO2C6UCaM"
      },
      "source": [
        "Reading & Processing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn_n8rPYUFOt"
      },
      "source": [
        "skin_df = pd.read_csv(os.path.join(base_skin_dir, 'HAM10000_metadata.csv'))\n",
        "\n",
        "# Creating New Columns for better readability\n",
        "\n",
        "skin_df['path'] = skin_df['image_id'].map(imageid_path_dict.get)\n",
        "skin_df['cell_type'] = skin_df['dx'].map(lesion_type_dict.get) \n",
        "skin_df['cell_type_idx'] = pd.Categorical(skin_df['cell_type']).codes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QJ_nWPBUIYC",
        "outputId": "10539303-11c0-48c7-af58-4013d051f13d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "Now lets see the sample of tile_df to look on newly made columns\n",
        "skin_df.head()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-a486ad271009>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Now lets see the sample of tile_df to look on newly made columns\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbRVbghwUSA_"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4raVkR85UN2A"
      },
      "source": [
        "skin_df.isnull().sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12tHnuhLUV1Y"
      },
      "source": [
        "skin_df['age'].fillna((skin_df['age'].mean()), inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAEV_Ly3UfwF"
      },
      "source": [
        "skin_df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVA5cWZUUiOq"
      },
      "source": [
        "print(skin_df.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVATQhquUpfs"
      },
      "source": [
        "**Step 5 : EDA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4qUa9RGUkGn",
        "outputId": "aa4bd376-6025-48cc-fda5-cd0106d462ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        }
      },
      "source": [
        "fig, ax1 = plt.subplots(1, 1, figsize= (10, 5))\n",
        "skin_df['cell_type'].value_counts().plot(kind='bar', ax=ax1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bf02c247ce8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mskin_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cell_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'skin_df' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeB7JnR6U1Ua"
      },
      "source": [
        "skin_df['dx_type'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geTQNf0yU31I"
      },
      "source": [
        "skin_df['localization'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzpkrq_2U6Jr"
      },
      "source": [
        "skin_df['age'].hist(bins=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD29uCzoU-D-"
      },
      "source": [
        "skin_df['sex'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeZPucv4VDDp"
      },
      "source": [
        "\n",
        "\n",
        "Now lets visualize agewise distribution of skin cancer types\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvfXCtgNVERk"
      },
      "source": [
        "ax = sns.boxplot('cell_type', 'age',data=skin_df)\n",
        "plt.setp(ax.get_xticklabels(), rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6jg1qahVTXm"
      },
      "source": [
        "Step 6: Loading and resizing of images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLwygtk3VHZj"
      },
      "source": [
        "skin_df['image'] = skin_df['path'].map(lambda x: np.asarray(Image.open(x).resize((100,75))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "969CIwXRVY20"
      },
      "source": [
        "# Checking the image size distribution\n",
        "skin_df['image'].map(lambda x: x.shape).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySdmllXRVpkN"
      },
      "source": [
        "# **Average color informations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt_DSWc4VgvV",
        "outputId": "cc0ab25d-c811-4227-824e-817350c82ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "rgb_info_df = skin_df.apply(lambda x: pd.Series({'{}_mean'.format(k): v for k, v in \n",
        "                                  zip(['Red', 'Green', 'Blue'], \n",
        "                                      np.mean(x['image'], (0, 1)))}),1)\n",
        "gray_col_vec = rgb_info_df.apply(lambda x: np.mean(x), 1)\n",
        "for c_col in rgb_info_df.columns:\n",
        "    rgb_info_df[c_col] = rgb_info_df[c_col]/gray_col_vec\n",
        "rgb_info_df['Gray_mean'] = gray_col_vec\n",
        "rgb_info_df.sample(3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-49f00b17518e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m rgb_info_df = skin_df.apply(lambda x: pd.Series({'{}_mean'.format(k): v for k, v in \n\u001b[0m\u001b[1;32m      2\u001b[0m                                   zip(['Red', 'Green', 'Blue'], \n\u001b[1;32m      3\u001b[0m                                       np.mean(x['image'], (0, 1)))}),1)\n\u001b[1;32m      4\u001b[0m \u001b[0mgray_col_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb_info_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrgb_info_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'skin_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDDCEmHLVlhx"
      },
      "source": [
        "for c_col in rgb_info_df.columns:\n",
        "    skin_df[c_col] = rgb_info_df[c_col].values # we cant afford a copy\n",
        "    \n",
        "sns.pairplot(skin_df[['Red_mean', 'Green_mean', 'Blue_mean', 'Gray_mean', 'cell_type']], \n",
        "             hue='cell_type', plot_kws = {'alpha': 0.5})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw3NDZ59V1jg"
      },
      "source": [
        "\n",
        "# **Step 7 : Train Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ikosa6HV4lu"
      },
      "source": [
        "features=skin_df.drop(columns=['cell_type_idx'],axis=1)\n",
        "target=skin_df['cell_type_idx']\n",
        "features.head()\n",
        "target.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz2wB0rtV8zJ"
      },
      "source": [
        "x_train_o, x_test_o, y_train_o, y_test_o = train_test_split(features, target, test_size=0.20,random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BAath8KV_zG"
      },
      "source": [
        "# Step 8 : **Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue6Mg4xrWD2N"
      },
      "source": [
        "x_train = np.asarray(x_train_o['image'].tolist())\n",
        "x_test = np.asarray(x_test_o['image'].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgv13H0gWIQ3"
      },
      "source": [
        "# **Step 9 : Label Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTzooobgWGJR"
      },
      "source": [
        "# Perform one-hot encoding on the labels\n",
        "y_train = to_categorical(y_train_o, num_classes = 7)\n",
        "y_test = to_categorical(y_test_o, num_classes = 7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYUiDHStWOUP"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo1Z-85ZWWPs"
      },
      "source": [
        "# **Step 10 : Splitting training and validation split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaQWeW92WY4q"
      },
      "source": [
        "x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size = 0.2, random_state = 2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsLRdDbUWbEw"
      },
      "source": [
        "# Reshape image in 3 dimensions (height = 75px, width = 100px , canal = 3)\n",
        "x_train = x_train.reshape(x_train.shape[0], *(75, 100, 3))\n",
        "x_test = x_test.reshape(x_test.shape[0], *(75, 100, 3))\n",
        "x_validate = x_validate.reshape(x_validate.shape[0], *(75, 100, 3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2rqeRRzWgHG"
      },
      "source": [
        "# **Step 11.1: Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCUb8DRiWdtK"
      },
      "source": [
        "# Set the CNN model \n",
        "# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
        "input_shape = (75, 100, 3)\n",
        "num_classes = 7\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding = 'Same',input_shape=input_shape))\n",
        "model.add(Conv2D(32,kernel_size=(3, 3), activation='relu',padding = 'Same',))\n",
        "model.add(MaxPool2D(pool_size = (2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.40))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_PD-4b1XTIA"
      },
      "source": [
        "\n",
        "Step 11.2: Using VGG16¶\n",
        "\n",
        "VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition”.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr2yNELTXNWv"
      },
      "source": [
        "conv_base = VGG19(weights = 'imagenet',\n",
        "                 include_top = False,\n",
        "                 input_shape = (75, 100, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvDDCwzvXbWM"
      },
      "source": [
        "conv_base.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIjrNG47XfsF"
      },
      "source": [
        "from keras import models\n",
        "num_classes = 7\n",
        "model1 = models.Sequential()\n",
        "model1.add(conv_base)\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(128, activation='relu'))\n",
        "model1.add(Dropout(0.5))\n",
        "model1.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx6PEXUDXi02"
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKKjduLMXnqF"
      },
      "source": [
        "#Step 11.3\n",
        "**ResNet50 with pre trained weights **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXQRKTP1X2Zm"
      },
      "source": [
        "base_model = ResNet50(weights='imagenet',\n",
        "                      include_top=False, input_shape=(75,100,3))\n",
        "model3=Sequential()\n",
        "model3.add(base_model)\n",
        "# Freeze the layers except the last 4 layers\n",
        "model3.add(Dropout(0.40))\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(512,activation='relu'))\n",
        "model3.add(Dropout(0.5))\n",
        "model3.add(Dense(7, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mduYbwfXX4-c"
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CPT99H1YAHj"
      },
      "source": [
        "# **Step 12: Setting Optimizer and Annealer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rMFqfsrX7MG"
      },
      "source": [
        "# Define the optimizer\n",
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XITUmkgGYHdu"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpointer = ModelCheckpoint('../input/best.hdf5', verbose=1,save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKkHvz9SYNve"
      },
      "source": [
        "def categorical_focal_loss(gamma=2., alpha=.25):\n",
        "    \"\"\"\n",
        "    Softmax version of focal loss.\n",
        "           m\n",
        "      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
        "          c=1\n",
        "      where m = number of classes, c = class and o = observation\n",
        "    Parameters:\n",
        "      alpha -- the same as weighing factor in balanced cross entropy\n",
        "      gamma -- focusing parameter for modulating factor (1-p)\n",
        "    Default value:\n",
        "      gamma -- 2.0 as mentioned in the paper\n",
        "      alpha -- 0.25 as mentioned in the paper\n",
        "    References:\n",
        "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
        "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
        "    Usage:\n",
        "     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
        "    \"\"\"\n",
        "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: A tensor of the same shape as `y_pred`\n",
        "        :param y_pred: A tensor resulting from a softmax\n",
        "        :return: Output tensor.\n",
        "        \"\"\"\n",
        "\n",
        "        # Scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "\n",
        "        # Clip the prediction value to prevent NaN's and Inf's\n",
        "        epsilon = K.epsilon()\n",
        "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
        "\n",
        "        # Calculate Cross Entropy\n",
        "        cross_entropy = -y_true * K.log(y_pred)\n",
        "\n",
        "        # Calculate Focal Loss\n",
        "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
        "\n",
        "        # Sum the losses in mini_batch\n",
        "        return K.sum(loss, axis=1)\n",
        "\n",
        "    return categorical_focal_loss_fixed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWxAXbA-YQhy"
      },
      "source": [
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "    Only computes a batch-wise average of recall.\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6gKqjfvYTY0"
      },
      "source": [
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer = optimizer , loss =\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model1.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model3.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1y1veU4YX_K"
      },
      "source": [
        "# Set a learning rate annealer\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
        "                                            patience=3, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SVTc9fqYbxY"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgqvLLO0YfQ9"
      },
      "source": [
        "# With data augmentation to prevent overfitting \n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.2, # Randomly zoom image \n",
        "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True # randomly flip images \n",
        "            )\n",
        "\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XckdrQfeYlrv"
      },
      "source": [
        " datagen2 = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "    \n",
        "datagen2.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i517ljr0Yopk"
      },
      "source": [
        "\n",
        "\n",
        "for X_batch, y_batch in datagen2.flow(x_train, y_train, batch_size=9):\n",
        "# grid of 3x3 images\n",
        "    for i in range(0, 9):\n",
        "        plt.subplot(330 + 1 + i)\n",
        "        plt.imshow(X_batch[i])\n",
        "\n",
        "    plt.show()\n",
        "    break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEFlhH75Yt1P"
      },
      "source": [
        "# **Step 13: Fitting the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMyCOOgkYzD7"
      },
      "source": [
        "13.1 NO NORMALIZATION, FOCAL LOSS 75x100 NO CLASS WEIGTHS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DOKhkARYwjX"
      },
      "source": [
        "class_weights={\n",
        "    0: 3.0, # akiec\n",
        "    1: 1.0, # bcc\n",
        "    2: 1.0, # bkl\n",
        "    3: 3.0, # df\n",
        "    4: 3.0, # mel # Try to make the model more sensitive to Melanoma.\n",
        "    5: 2.0, # nv\n",
        "    6: 1.0, # vasc\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWovRxyRY7dH"
      },
      "source": [
        "# Fit the model\n",
        "epochs = 30\n",
        "batch_size = 10\n",
        "history = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = (x_validate,y_validate),\n",
        "                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size\n",
        "                              , callbacks=[learning_rate_reduction])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAdFRBR2ZAIL"
      },
      "source": [
        "history1 = model1.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n",
        "                      epochs = epochs, validation_data = (x_validate,y_validate),\n",
        "                      verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                      callbacks=[learning_rate_reduction])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GGcJj83ZDrx"
      },
      "source": [
        "history3 = model3.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = (x_validate,y_validate),\n",
        "                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size\n",
        "                              , callbacks=[learning_rate_reduction])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDdXU7gHZJ-s"
      },
      "source": [
        "# **Step 14: Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbJPXYMDZO_X"
      },
      "source": [
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
        "loss_v, accuracy_v = model.evaluate(x_validate, y_validate, verbose=1)\n",
        "print(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\n",
        "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
        "model.save(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}